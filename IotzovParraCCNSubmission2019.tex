%Ivan Iotzov and Lucas Parra submission to CCN 2019, Berlin

\documentclass[10pt,letterpaper]{article}

\usepackage{ccn}
\usepackage{pslatex}
\usepackage{apacite}
\usepackage{graphicx}

\graphicspath{ {./figures/} }


\title{Using EEG to Predict Speech Intelligibility}
 
\author{{\large \bf Ivan Iotzov (iiotzov@gradcenter.cuny.edu)} \\
  Department of Biomedical Engineering, 160 Convent Avenue\\
New York, NY 10030, U.S.A.
  \AND {\large \bf Lucas C. Parra (parra@ccny.cuny.edu)} \\
  Department of Biomedical Engineering, 160 Convent Avenue\\
New York, NY 10030, U.S.A.}


\begin{document}

\maketitle


\section{Abstract}
{
\bf
Speech signals have the ability to entrain brain activity to the rapid 
fluctuations found in speech sounds. This entrainment can be measured 
using electroencephalographic (EEG) recordings and is strong enough to allow 
discrimination between attended and unattended speech sources. In this study,
we investigated whether these entrainment responses can be used to measure 
how intelligible a speech signal is to a subject. We hypothesized that when 
intelligibility is degraded, attention wanes and the stimulus-response 
correlation will decrease. To test this, we measured a listener's ability 
to detect words in noisy, natural speech while recording brain activity using 
EEG. We altered intelligibility by presenting congruent or incongruent video 
of the speaker along with their speech. For almost all subjects, word 
detection performance improved in the congruent condition and this improvement 
coincided with an increase in stimulus-response correlation. We conclude that 
simultaneous recordings of perceived sound and EEG activity may represent a 
practical tool to assess speech intelligibility, specifically in the context 
of hearing aid devices.
}

\begin{quote}
\small
\textbf{Keywords:} 
EEG; stimulus-response correlation; 
speech comprehension; speech intelligibility
\end{quote}

\section{Introduction}

While listening to sounds, brain activity follows the fast fluctuations of 
the acoustic stimulus \cite{Ding2014a,Haegens2018}. This effect can also
be observed in subjects listening to speech signals, 
where EEG and MEG signals have been shown to correlate 
with fluctuations in signal amplitude and spectral content 
\cite{Luo2007,Horton2014}. This stimulus-driven brain activity has been linked
to attention \cite{ZionGolumbic2013,OSullivan2015}, in particular in 
multi-speaker scenarios where these correlations are thought to reflect 
the listener's ability to follow the attended speech stream. We see similar 
findings in studies of noisy speech, where the clean speech envelope still 
shows correlation with the brain response \cite{Ding2013,Vanthornhout2017}.
We attribute this
speech tracking phenomenon to an exogenous stimulus-driven process due to the 
consistent responses elicited by a speech stimulus across subjects 
\cite{Ki2016,Cohen2017}. 

A consistent 
confound in previous research has been that speech intelligibility is 
modulated by altering various properties of the stimulus. This makes
it difficult to determine whether changes in speech tracking are 
caused by genuine changes in auditory processing or merely a reflection of the
altered stimulus. We have attempted to mitigate this confound by keeping 
the stimulus unchanged and modulating speech intelligibility through 
visual cues. The audio presented to subjects in our two experimental conditions
is identical and we modulate intelligibility by presenting visuals that 
are either congruent (i.e. the mouth of the speaker and the heard audio align)
or incongruent.

\begin{figure}[h]
  \begin{center}
    \includegraphics[width=\columnwidth,height=5.5cm]{Figure3a}
  \end{center}
  \caption{Visualization of the CCA model showing spatial (top)
  and temporal (bottom) EEG response functions. SRC values shown
  are an average over all subjects and conditions.}
  \label{srcModel}
\end{figure}



\section{Methods}

\subsection{Stimulus Presentation and Behavioral Measures}

  The stimuli used in this experiment were previously used in other 
  speech tracking experiments with EEG \cite{Crosse2015,Crosse2016}.
  The stimuli consist of 120 audiovisual talking head clips of President
  Barack Obama discussing the Affordable Care Act, each 60s long. In total, 
  there were four stimulus conditions combining -9 dB/ -6 dB noise and 
  congruent/incongruent visuals in a 2 x 2 design. Subjects were presented
  with 30 stimuli in each of the four conditions, for a total of 120
  trials that took place over 2 experimental sessions.

  Before each 60s trial, subjects were presented with a `target word' and 
  were instructed to press a button whenever they heard the target. Target 
  words were selected so that each word occurs the same number of times
  in all four conditions. Responses within 1.5s of word presentation 
  were coded as correct detection and any responses outside of this window
  were coded as false alarms. Correct detections can be reported either 
  relative to the total number of target words (detection) or relative 
  to the total number of responses (precision). In order to capture 
  both of these properties, we report behavioral performance using the 
  F1 score, which is the harmonic mean of detection and precision.
  

\subsection{EEG and Stimulus Processing}

  EEG was recorded from 20 healthy subjects with normal hearing
  using a BioSemi Active II amplifier with 64 electrodes, in addition to 6
  electrooculogram (EOG) electrodes. The EEG was sampled at 512 Hz and 
  later downsampled to the framerate of the video presentation (30 Hz).
  Before correlation to the stimulus, the EEG signal was preprocessed as 
  follows: the initial value was subtracted from the data to remove the 
  DC offset. A high-pass 5th order Butterworth filter with a cutoff of 
  0.5 Hz was applied. The signal from the 6 EOG electrodes was regressed 
  out using a least-squares algorithm. Finally, artifacts and channels 
  with recording quality issues were removed.

  The steps used to calculate the stimulus amplitude envelope are as follows:
  the sound amplitude envelope is calculated as the absolute value of the 
  analytic signal after a Hilbert transform fo the raw mono sound signal 
  at its original sampling rate of 48 kHz. The result is then downsampled
  to the framerate of the video (30 Hz) and z-scored. Then, a Toeplitz matrix
  with 30 columns is constructed to capture up to a 1s delay.
  
\subsection{Stimulus-Response Correlation (SRC)}

  % \begin{figure}[]
  %   \begin{center}
  %     \includegraphics[width=\columnwidth,height=5.5cm]{Figure3a}
  %   \end{center}
  %   \caption{Visualization of the CCA model showing spatial (top)
  %   and temporal (bottom) EEG response functions. SRC values shown
  %   are an average over all subjects and conditions.}
  %   \label{srcModel}
  % \end{figure}

  The models used in much of the speech tracking literature 
  are typically encoding or 
  decoding models. The encoding approach uses various features of the 
  stimulus to predict the brain response, while the decoding approach works
  from the brain response and attempts to reconstruct the stimulus.
  Here, we used a 
  hybrid encoding and decoding approach \cite{Dmochowski2017}. The model 
  attempts to maximize the correlation between the encoded stimulus 
  $\hat{u}(t)$ and the decoded response $\hat{v}(t)$. The two signals 
  are defined as:
    $$\hat{u}(t) = h(t) * s(t)$$
    $$\hat{v}(t) = \sum_{i} w_ir_i(t)$$
  where $s(t)$ is the sound amplitude envelope at time $t$, $h(t)$ is the 
  encoding filter applied to the stimulus signal, * represents convolution, 
  $w_i$ are the weights applied to the neural response, and $r_i(t)$ is the 
  EEG signal value at time $t$ at electrode $i$. The model parameters 
  $h(t)$ and $w_i$ are found using canonical correlation analysis (CCA).
  CCA looks to maximize the correlation between the encoded stimulus and the 
  decoded brain response by computing several components that each capture
  some portion of the correlated signal. The stimulus-response correlation (SRC)
  that we report is the sum of the correlation of $\hat{u}(t)$ and $\hat{v}(t)$
  for the first three components. For a more full discussion of the method 
  please see \cite{Dmochowski2017}.

  The model is trained using data from all subjects in all conditions. 
  The resulting spatial and temporal response functions can be seen in 
  Fig. \ref{srcModel}. The model is then used to separately compute SRC for each 
  subject in each of the four conditions. Statistical significance of SRC 
  values is estimated by comparing values to those produced by 
  correlating 1000 sets of circularly shuffled EEG data with the stimuli using 
  the same procedure as the normal EEG data.
  

% \subsection{Figures}

% Make sure that the artwork can be printed well (e.g. dark colors) and that 
% the figures make understanding the paper easy.
%  Number figures sequentially, placing the figure
% number and caption, in 10~point, after the figure with one line space
% above the caption and one line space below it, as in
% Figure~\ref{sample-figure}. If necessary, leave extra white space at
% the bottom of the page to avoid splitting the figure and figure
% caption. You may float figures to the top or bottom of a column, or
% set wide figures across both columns.

% \begin{figure}[ht]
% \begin{center}
% \fbox{CCN figure}
% \end{center}
% \caption{This is a figure.} 
% \label{sample-figure}
% \end{figure}


\section{Results}

\subsection{Congruent Visual Speech Increases Word Detection}

  Subject performance on the behavioral task was quantified in terms of the 
  F1 score (described in Methods above). We performed a two-way repeated 
  measures ANOVA on these scores, considering factors of noise level and 
  congruency. We found very large effects on behavior of both noise level 
  $[F(1,19) = 284.9, p = 6 x 10^{-13}]$ and congruency
  $[F(1, 19) = 917.4, p = 1 x 10^{-17}]$, as well as for the interaction 
  between noise level and congruency $[F(1, 19) = 118.1, p = 1 x 10^{-9}]$.

  These results are in line with our expectations, as the effects of visual 
  information on the perception of noisy speech as well characterized 
  \cite{Ross2007}. Additionally, the results indicate that we successfully 
  manipulated the intelligibility of the audiovisual speech without 
  altering the auditory portion of the stimulus.
  For a visual representation of these results, please see Fig. \ref{behavior}.

  \begin{figure}[h]
    \begin{center}
      \includegraphics[]{Figure2c}
    \end{center}
    \caption{Performance on the behavioral word detection
    task for each subject in each condition (-6dB congruent, 
    -6dB incongruent, -9dB congruent, -9dB incongruent) 
    reported as F1 statistic.}
    \label{behavior}
  \end{figure}

\subsection{Congruent Visual Speech Increases Stimulus-Response Correlation}

  \begin{figure}[]
    \begin{center}
      \includegraphics[width=0.8\columnwidth,height=6cm]{Figure4}
    \end{center}
    \caption{SRC scores for each subject in all conditions.
    Performance within each noise condition is connected 
    with a line.}
    \label{srcResults}
  \end{figure}

  Using the hybrid approach described above, we used CCA 
  to relate the auditory amplitude envelope at various time delays to the 
  EEG signal across various electrodes. This process results in a number of 
  correlated components, of which we only examine the three most highly 
  correlated. The resulting spatial and temporal 
  response functions can be seen in Fig. \ref{srcModel}. The correlation 
  values found are small, but are significant given the large amount of 
  data collected \textit{(r = 0.039, 0.025, 0.009, respectively for 
  first 3 SRC components, p < 0.001 using shuffle statistics)}.
  Overall SRC values are reported as the sum of these first three components, 
  calculated separately for each subject and condition 
  (See Fig. \ref{srcResults}).

  We performed a two-way repeated measures ANOVA on these SRC values and found 
  a very significant effect for both noise level 
  $[F(1, 19) = 44.54, p = 2 x 10^{-6}]$ and congruency 
  $[F(1, 19) = 109.4, p = 2 x 10^{-9}]$. In contrast with the behavioral 
  results, we did not find a significant interaction between the two effects,
  suggesting that the effect of congruency on SRC is equally strong at both 
  noise levels. 
  To test the efficacy of correlating the noisy speech signal to the EEG
  responses, we repeated the analysis using the envelope of the 
  clean speech and the envelope of the noise alone, 
  instead of the mixed noisy speech as above.

  The results of this secondary analysis showed increased 
  SRC values for the clean 
  speech \textit{(r = 0.05, 0.03, 0.01, respectively for first 3 components,
  p < 0.001 using shuffle statistics)} 
  compared to the noisy speech \textit{(r = 0.039, 0.025, 0.009, respectively
  for first 3 SRC components)}, as well as strong effects of noise level 
  $[F(1, 19) = 52.53, p = 7 x 10^{-7}]$ and congruency
  $[F(1, 19) = 54.43, p = 5 x 10^{-7}]$. The SRC for the noise-only envelope 
  was much weaker \textit{(r = 0.01, 0.0047, 0.0019, p < 0.001, p = 0.001, 
  p = 0.14, respectively for first 3 components, p-values calculated through
  shuffle statistics)}, and we did not find a significant effect for noise 
  level, nor for congruency. This result conflicts with our initial hypothesis,
  and suggests that listeners were able to extract speech from the noisy 
  environment and were not `tracking' the noise in the same way as the speech.
  However, considering the small differences between the noisy speech and 
  clean speech results, this result does validate our approach using the 
  noisy speech, which is the only input available in real-world scenarios.

  We visually relate behavioral task performance to SRC
  in Fig. \ref{srcVsBehavior}a. For the majority of subjects, an increase 
  in SRC coincides with increased performance on the behavioral task 
  (indicated by positive slope in Fig. \ref{srcVsBehavior}a). Fig. 
  \ref{srcVsBehavior}b is a different view of the same result, showing that
  the change in behavioral performance and SRC have the same sign for most 
  subjects \textit{(p = 0.0026, p = 0.0004 for -9dB and -6dB respectively, 
  sign test)}.

  \begin{figure*}[]
    \begin{center}
      \includegraphics[scale=0.8]{Figure5ab}
    \end{center}
    \caption{(a) Comparison between behavioral word detection performance
    and SRC for each subject in all conditions. (b) Difference between
    congruent and incongruent conditions within each noise level for
    each subject. Points in the first quadrant indicate that gains in
    one measure coincide with gains in the other.}
    \label{srcVsBehavior}
  \end{figure*}


\section{Conclusion}

  Our main finding, that improved speech perception coincides with an increase 
  in stimulus-response correlation, is consistent with much of the previous 
  research into speech tracking
  \cite{Ding2013,Peelle2013,Vanthornhout2017,Crosse2015}. 
  We extend this previous work by using incongruent audiovisual speech as a 
  control condition in noise, and demonstrated that this effect is correlated 
  with gains in behavioral performance measures within individual subjects.

  % There is ongoing debate in the speech tracking literature about whether 
  % these correlated EEG responses to speech represent an active parsing of 
  % hierarchical semantic language features \cite{Ding2015} or are a simpler 
  % response to acoustic features of the words presented \cite{Frank2018}.
  % Our results suggest that when a listener successfully detects and attends 
  % to speech in a noisy environment, the evoked EEG responses are stronger than 
  % when the listener fails to detect the speech. We do not attempt to make a
  % causal claim about whether word detection elicits the evoked EEG responses 
  % or vice versa, as waning attention may account for the diminished SRC. 
  % Further studies would be needed to disentangle the role that attention plays
  % here.

  The motivation for our work was to find an objective assessment for the 
  intelligibility of speech in the context of hearing aids. There are several
  novel contributions that our work offers in this context. First, we have 
  shown the reliable ability to predict intelligibility within individual 
  subjects, which is critical in tuning a hearing aid for an individual. 
  Second, because we did not alter the auditory stimulus, confounds surrounding
  the alteration of the auditory stimulus have been eliminated. The changes 
  in SRC observed here are thus likely due to the processing of the auditory
  stimulus by the subject, and not a result of changes to the stimulus itself.
  Finally, in contrast to previous work, our SRC measure does not require
  access to the clean speech in order to make predictions about intelligibility.
  In practical scenarios, access to the clean speech is impossible, thus making
  an approach that can work within noisy contexts essential.


\bibliographystyle{apacite}

\setlength{\bibleftmargin}{.125in}
\setlength{\bibindent}{-\bibleftmargin}

\bibliography{IotzovParraCCNSubmission2019}


\end{document}
