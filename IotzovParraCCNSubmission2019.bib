@article{Cohen2017,
    abstract = {It is said that we lose track of time -that " time flies " -when we are engrossed in a story. How does engagement with the story cause this distorted perception of time, and what are its neural correlates? People commit both time and attentional resources to an engaging stimulus. For narrative videos, attentional engagement can be represented as the level of similarity between the electroencephalographic responses of different viewers. Here we show that this measure of neural engagement predicted the duration of time that viewers were willing to commit to narrative videos. Contrary to popular wisdom, engagement did not distort the average perception of time duration. Rather, more similar brain responses resulted in a more uniform perception of time across viewers. These findings suggest that by capturing the attention of an audience, narrative videos bring both neural processing and the subjective perception of time into synchrony.},
    author = {Cohen, Samantha S. and Henin, Simon and Parra, Lucas C.},
    doi = {10.1038/s41598-017-04402-4},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Henin, Parra - 2017 - Engaging narratives evoke similar neural activity and lead to similar time perception(2).pdf:pdf},
    issn = {20452322},
    journal = {Scientific Reports},
    mendeley-groups = {First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {1},
    pages = {1--10},
    publisher = {Springer US},
    title = {{Engaging narratives evoke similar neural activity and lead to similar time perception}},
    volume = {7},
    year = {2017}
}
@article{Crosse2015,
    abstract = {UNLABELLED: Congruent audiovisual speech enhances our ability to comprehend a speaker, even in noise-free conditions. When incongruent auditory and visual information is presented concurrently, it can hinder a listener's perception and even cause him or her to perceive information that was not presented in either modality. Efforts to investigate the neural basis of these effects have often focused on the special case of discrete audiovisual syllables that are spatially and temporally congruent, with less work done on the case of natural, continuous speech. Recent electrophysiological studies have demonstrated that cortical response measures to continuous auditory speech can be easily obtained using multivariate analysis methods. Here, we apply such methods to the case of audiovisual speech and, importantly, present a novel framework for indexing multisensory integration in the context of continuous speech. Specifically, we examine how the temporal and contextual congruency of ongoing audiovisual speech affects the cortical encoding of the speech envelope in humans using electroencephalography. We demonstrate that the cortical representation of the speech envelope is enhanced by the presentation of congruent audiovisual speech in noise-free conditions. Furthermore, we show that this is likely attributable to the contribution of neural generators that are not particularly active during unimodal stimulation and that it is most prominent at the temporal scale corresponding to syllabic rate (2-6 Hz). Finally, our data suggest that neural entrainment to the speech envelope is inhibited when the auditory and visual streams are incongruent both temporally and contextually.$\backslash$n$\backslash$nSIGNIFICANCE STATEMENT: Seeing a speaker's face as he or she talks can greatly help in understanding what the speaker is saying. This is because the speaker's facial movements relay information about what the speaker is saying, but also, importantly, when the speaker is saying it. Studying how the brain uses this timing relationship to combine information from continuous auditory and visual speech has traditionally been methodologically difficult. Here we introduce a new approach for doing this using relatively inexpensive and noninvasive scalp recordings. Specifically, we show that the brain's representation of auditory speech is enhanced when the accompanying visual speech signal shares the same timing. Furthermore, we show that this enhancement is most pronounced at a time scale that corresponds to mean syllable length.},
    author = {Crosse, M. J. and Butler, J. S. and Lalor, E. C.},
    doi = {10.1523/JNEUROSCI.1829-15.2015},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Crosse, Butler, Lalor - 2015 - Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditi.pdf:pdf},
    isbn = {0270-6474},
    issn = {0270-6474},
    journal = {Journal of Neuroscience},
    mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {42},
    pages = {14195--14204},
    pmid = {26490860},
    title = {{Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditions}},
    volume = {35},
    year = {2015}
}
@article{Crosse2016,
    abstract = {UNLABELLED Speech comprehension is improved by viewing a speaker's face, especially in adverse hearing conditions, a principle known as inverse effectiveness. However, the neural mechanisms that help to optimize how we integrate auditory and visual speech in such suboptimal conversational environments are not yet fully understood. Using human EEG recordings, we examined how visual speech enhances the cortical representation of auditory speech at a signal-to-noise ratio that maximized the perceptual benefit conferred by multisensory processing relative to unisensory processing. We found that the influence of visual input on the neural tracking of the audio speech signal was significantly greater in noisy than in quiet listening conditions, consistent with the principle of inverse effectiveness. Although envelope tracking during audio-only speech was greatly reduced by background noise at an early processing stage, it was markedly restored by the addition of visual speech input. In background noise, multisensory integration occurred at much lower frequencies and was shown to predict the multisensory gain in behavioral performance at a time lag of ∼250 ms. Critically, we demonstrated that inverse effectiveness, in the context of natural audiovisual (AV) speech processing, relies on crossmodal integration over long temporal windows. Our findings suggest that disparate integration mechanisms contribute to the efficient processing of AV speech in background noise. SIGNIFICANCE STATEMENT The behavioral benefit of seeing a speaker's face during conversation is especially pronounced in challenging listening environments. However, the neural mechanisms underlying this phenomenon, known as inverse effectiveness, have not yet been established. Here, we examine this in the human brain using natural speech-in-noise stimuli that were designed specifically to maximize the behavioral benefit of audiovisual (AV) speech. We find that this benefit arises from our ability to integrate multimodal information over longer periods of time. Our data also suggest that the addition of visual speech restores early tracking of the acoustic speech signal during excessive background noise. These findings support and extend current mechanistic perspectives on AV speech perception.},
    author = {Crosse, M. J. and {Di Liberto}, G. M. and Lalor, E. C.},
    doi = {10.1523/JNEUROSCI.1396-16.2016},
    isbn = {0270-6474},
    issn = {0270-6474},
    journal = {Journal of Neuroscience},
    mendeley-groups = {obama paper 2018,ccn 2019 submission},
    number = {38},
    pages = {9888--9895},
    pmid = {27656026},
    title = {{Eye Can Hear Clearly Now: Inverse Effectiveness in Natural Audiovisual Speech Processing Relies on Long-Term Crossmodal Temporal Integration}},
    volume = {36},
    year = {2016}
}
@article{Ding2013,
abstract = {Speech recognition is remarkably robust to the listening background, even when the energy of background sounds strongly overlaps with that of speech. How the brain transforms the corrupted acoustic signal into a reliable neural representation suitable for speech recognition, however, remains elusive. Here, we hypothesize that this transformation is performed at the level of auditory cortex through adaptive neural encoding, and we test the hypothesis by recording, using MEG, the neural responses of human subjects listening to a narrated story. Spectrally matched stationary noise, which has maximal acoustic overlap with the speech, is mixed in at various intensity levels. Despite the severe acoustic interference caused by this noise, it is here demonstrated that low-frequency auditory cortical activity is reliably synchronized to the slow temporal modulations of speech, even when the noise is twice as strong as the speech. Such a reliable neural representation is maintained by intensity contrast gain control and by adaptive processing of temporal modulations at different time scales, corresponding to the neural {\{}delta{\}} and {\{}theta{\}} bands. Critically, the precision of this neural synchronization predicts how well a listener can recognize speech in noise, indicating that the precision of the auditory cortical representation limits the performance of speech recognition in noise. Together, these results suggest that, in a complex listening environment, auditory cortex can selectively encode a speech stream in a background insensitive manner, and this stable neural representation of speech provides a plausible basis for background-invariant recognition of speech.},
author = {Ding, N. and Simon, J. Z.},
doi = {10.1523/jneurosci.5297-12.2013},
issn = {0270-6474},
journal = {Journal of Neuroscience},
mendeley-groups = {ccn 2019 submission},
number = {13},
pages = {5728--5735},
title = {{Adaptive Temporal Encoding Leads to a Background-Insensitive Cortical Representation of Speech}},
volume = {33},
year = {2013}
}
@article{Ding2014a,
    abstract = {Auditory cortical activity is entrained to the temporal envelope of speech, which corresponds to the syllabic rhythm of speech. Such entrained cortical activity can be measured from subjects naturally listening to sentences or spoken passages, providing a reliable neural marker of online speech processing. A central question still remains to be answered about whether cortical entrained activity is more closely related to speech perception or non-speech-specific auditory encoding. Here, we review a few hypotheses about the functional roles of cortical entrainment to speech, e.g., encoding acoustic features, parsing syllabic boundaries, and selecting sensory information in complex listening environments. It is likely that speech entrainment is not a homogeneous response and these hypotheses apply separately for speech entrainment generated from different neural sources. The relationship between entrained activity and speech intelligibility is also discussed. A tentative conclusion is that theta-band entrainment (4-8 Hz) encodes speech features critical for intelligibility while delta-band entrainment (1-4 Hz) is related to the perceived, non-speech-specific acoustic rhythm. To further understand the functional properties of speech entrainment, a splitter's approach will be needed to investigate (1) not just the temporal envelope but what specific acoustic features are encoded and (2) not just speech intelligibility but what specific psycholinguistic processes are encoded by entrained cortical activity. Similarly, the anatomical and spectro-temporal details of entrained activity need to be taken into account when investigating its functional properties.},
    author = {Ding, Nai and Simon, Jonathan Z.},
    doi = {10.3389/fnhum.2014.00311},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2014 - Cortical entrainment to continuous speech functional roles and interpretations.pdf:pdf},
    issn = {1662-5161},
    journal = {Frontiers in Human Neuroscience},
    mendeley-groups = {First Exam Citations,Alice+Pieman,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    pmid = {24904354},
    title = {{Cortical entrainment to continuous speech: functional roles and interpretations}},
    volume = {8},
    year = {2014}
}
@article{Dmochowski2017,
    abstract = {In neuroscience, stimulus-response relationships have traditionally been analyzed using either encoding or decoding models. Here we propose a hybrid approach that decomposes neural activity into multiple components, each representing a portion of the stimulus. The technique is implemented via canonical correlation analysis (CCA) by temporally filtering the stimulus (encoding) and spatially filtering the neural responses (decoding) such that the resulting components are maximally correlated. In contrast to existing methods, this approach recovers multiple correlated stimulus-response pairs, and thus affords a richer, multidimensional analysis of neural representations. We first validated the technique's ability to recover multiple stimulus-driven components using electroencephalographic (EEG) data simulated with a finite element model of the head. We then applied the technique to real EEG responses to auditory and audiovisual narratives experienced identically across subjects, as well as uniquely experienced video game play. During narratives, both auditory and visual stimulus-response correlations (SRC) were modulated by attention and tracked inter-subject correlations. During video game play, SRC varied with game difficulty and the presence of a dual task. Interestingly, the strongest component extracted for visual and auditory features of film clips had nearly identical spatial distributions, suggesting that the predominant encephalographic response to naturalistic stimuli is supramodal. The diversity of these findings demonstrates the utility of measuring multidimensional SRC via hybrid encoding-decoding.},
    author = {Dmochowski, Jacek P. and Ki, Jason J. and DeGuzman, Paul and Sajda, Paul and Parra, Lucas C.},
    doi = {10.1016/j.neuroimage.2017.05.037},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dmochowski et al. - 2017 - Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity.pdf:pdf},
    issn = {10959572},
    journal = {NeuroImage},
    mendeley-groups = {ccn 2019 submission},
    number = {May},
    pages = {1--13},
    publisher = {Elsevier},
    title = {{Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity}},
    year = {2017}
}
@article{Haegens2018,
    abstract = {Here we review the role of brain oscillations in sensory processing. We examine the idea that neural entrainment of intrinsic oscillations underlies the processing of rhythmic stimuli in the context of simple isochronous rhythms as well as in music and speech. This has been a topic of growing interest over recent years; however, many issues remain highly controversial: how do fluctuations of intrinsic neural oscillations—both spontaneous and entrained to external stimuli—affect perception, and does this occur automatically or can it be actively controlled by top-down factors? Some of the controversy in the literature stems from confounding use of terminology. Moreover, it is not straightforward how theories and findings regarding isochronous rhythms generalize to more complex, naturalistic stimuli, such as speech and music. Here we aim to clarify terminology, and distinguish between different phenomena that are often lumped together as reflecting “neural entrainment” but may actually vary in their mechanistic underpinnings. Furthermore, we discuss specific caveats and confounds related to making inferences about oscillatory mechanisms from human electrophysiological data.},
    author = {Haegens, Saskia and {Zion Golumbic}, Elana},
    doi = {10.1016/j.neubiorev.2017.12.002},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haegens, Zion Golumbic - 2018 - Rhythmic facilitation of sensory processing A critical review.pdf:pdf},
    issn = {18737528},
    journal = {Neuroscience and Biobehavioral Reviews},
    keywords = {Entrainment,Music,Oscillations,Rhythm,Speech,Synchronization},
    mendeley-groups = {First Exam Citations/Ch4,obama paper 2018,First Exam Citations,First Exam Citations/oscillation,First Exam Citations/presentation,ccn 2019 submission},
    number = {December 2017},
    pages = {150--165},
    pmid = {29223770},
    publisher = {Elsevier},
    title = {{Rhythmic facilitation of sensory processing: A critical review}},
    volume = {86},
    year = {2018}
}
@article{Horton2014,
    abstract = {Objective. Recent studies have shown that auditory cortex better encodes the envelope of attended speech than that of unattended speech during multi-speaker ('cocktail party') situations. We investigated whether these differences were sufficiently robust within single-trial electroencephalographic (EEG) data to accurately determine where subjects attended. Additionally, we compared this measure to other established EEG markers of attention. Approach. High-resolution EEG was recorded while subjects engaged in a two-speaker 'cocktail party' task. Cortical responses to speech envelopes were extracted by cross-correlating the envelopes with each EEG channel. We also measured steady-state responses (elicited via high-frequency amplitude modulation of the speech) and alpha-band power, both of which have been sensitive to attention in previous studies. Using linear classifiers, we then examined how well each of these features could be used to predict the subjects' side of attention at various epoch lengths. Main results. We found that the attended speaker could be determined reliably from the envelope responses calculated from short periods of EEG, with accuracy improving as a function of sample length. Furthermore, envelope responses were far better indicators of attention than changes in either alpha power or steady-state responses. Significance. These results suggest that envelope-related signals recorded in EEG data can be used to form robust auditory BCI's that do not require artificial manipulation (e.g., amplitude modulation) of stimuli to function.},
    archivePrefix = {arXiv},
    arxivId = {NIHMS150003},
    author = {Horton, Cort and Srinivasan, Ramesh and D'Zmura, Michael},
    doi = {10.1088/1741-2560/11/4/046015},
    eprint = {NIHMS150003},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horton, Srinivasan, D'Zmura - 2014 - Envelope responses in single-trial EEG indicate attended speaker in a ‘cocktail party'.pdf:pdf},
    isbn = {2156623929},
    issn = {1741-2560},
    journal = {Journal of Neural Engineering},
    keywords = {alpha lateralization,brain,computer interfaces,selective attention,speech envelopes,steady-state responses},
    mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {4},
    pages = {046015},
    pmid = {24963838},
    publisher = {IOP Publishing},
    title = {{Envelope responses in single-trial EEG indicate attended speaker in a ‘cocktail party'}},
    volume = {11},
    year = {2014}
}
@article{Ki2016,
    abstract = {UNLABELLED: Attentional engagement is a major determinant of how effectively we gather information through our senses. Alongside the sheer growth in the amount and variety of information content that we are presented with through modern media, there is increased variability in the degree to which we "absorb" that information. Traditional research on attention has illuminated the basic principles of sensory selection to isolated features or locations, but it provides little insight into the neural underpinnings of our attentional engagement with modern naturalistic content. Here, we show in human subjects that the reliability of an individual's neural responses with respect to a larger group provides a highly robust index of the level of attentional engagement with a naturalistic narrative stimulus. Specifically, fast electroencephalographic evoked responses were more strongly correlated across subjects when naturally attending to auditory or audiovisual narratives than when attention was directed inward to a mental arithmetic task during stimulus presentation. This effect was strongest for audiovisual stimuli with a cohesive narrative and greatly reduced for speech stimuli lacking meaning. For compelling audiovisual narratives, the effect is remarkably strong, allowing perfect discrimination between attentional state across individuals. Control experiments rule out possible confounds related to altered eye movement trajectories or order of presentation. We conclude that reliability of evoked activity reproduced across subjects viewing the same movie is highly sensitive to the attentional state of the viewer and listener, which is aided by a cohesive narrative.$\backslash$n$\backslash$nSIGNIFICANCE STATEMENT: Modern technology has brought about a monumental surge in the availability of information. There is a pressing need for ways to measure and characterize attentional engagement and understand the causes of its waxing and waning in natural settings. The present study demonstrates that the degree of an individual's attentional engagement with naturalistic narrative stimuli is strongly predicted by the degree of similarity of his/her neural responses to a wider group. The effect is so strong that it enables perfect classification of attentional state across individuals for some narrative stimuli. As modern information content continues to diversify, such direct neural metrics will become increasingly important to manage and evaluate its effects on both the healthy and disordered human brain.},
    author = {Ki, J. J. and Kelly, S. P. and Parra, L. C.},
    doi = {10.1523/JNEUROSCI.2942-15.2016},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ki, Kelly, Parra - 2016 - Attention Strongly Modulates Reliability of Neural Responses to Naturalistic Narrative Stimuli.pdf:pdf},
    isbn = {0270-6474},
    issn = {0270-6474},
    journal = {Journal of Neuroscience},
    keywords = {attention,inter-subject correlation,naturalistic narrative stimuli},
    mendeley-groups = {Alice+Pieman,cns2018 poster,obama paper 2018,ccn 2019 submission},
    number = {10},
    pages = {3092--3101},
    pmid = {26961961},
    title = {{Attention Strongly Modulates Reliability of Neural Responses to Naturalistic Narrative Stimuli}},
    volume = {36},
    year = {2016}
}
@article{Luo2007,
    abstract = {How natural speech is represented in the auditory cortex constitutes a major challenge for cognitive neuroscience. Although many single-unit and neuroimaging studies have yielded valuable insights about the processing of speech and matched complex sounds, the mechanisms underlying the analysis of speech dynamics in human auditory cortex remain largely unknown. Here, we show that the phase pattern of theta band (4-8 Hz) responses recorded from human auditory cortex with magnetoencephalography (MEG) reliably tracks and discriminates spoken sentences and that this discrimination ability is correlated with speech intelligibility. The findings suggest that an ???200 ms temporal window (period of theta oscillation) segments the incoming speech signal, resetting and sliding to track speech dynamics. This hypothesized mechanism for cortical speech analysis is based on the stimulus-induced modulation of inherent cortical rhythms and provides further evidence implicating the syllable as a computational primitive for the representation of spoken language. ?? 2007 Elsevier Inc. All rights reserved.},
    archivePrefix = {arXiv},
    arxivId = {NIHMS150003},
    author = {Luo, Huan and Poeppel, David},
    doi = {10.1016/j.neuron.2007.06.004},
    eprint = {NIHMS150003},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Poeppel - 2007 - Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex.pdf:pdf},
    isbn = {2122633255},
    issn = {08966273},
    journal = {Neuron},
    keywords = {SYSNEURO},
    mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {6},
    pages = {1001--1010},
    pmid = {17582338},
    title = {{Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex}},
    volume = {54},
    year = {2007}
}
@article{OSullivan2015,
    abstract = {How humans solve the cocktail party problem remains unknown. However, progress has been made recently thanks to the realization that cortical activity tracks the amplitude envelope of speech. This has led to the development of regression methods for studying the neurophysiology of continuous speech. One such method, known as stimulus-reconstruction, has been successfully utilized with cortical surface recordings and magnetoencephalography (MEG). However, the former is invasive and gives a relatively restricted view of processing along the auditory hierarchy, whereas the latter is expensive and rare. Thus it would be extremely useful for research in many populations if stimulus-reconstruction was effective using electroencephalography (EEG), a widely available and inexpensive technology. Here we show that single-trial (≈60 s) unaveraged EEG data can be decoded to determine attentional selection in a naturalistic multispeaker environment. Furthermore, we show a significant correlation between our EEG-based measure of attention and performance on a high-level attention task. In addition, by attempting to decode attention at individual latencies, we identify neural processing at ∼200 ms as being critical for solving the cocktail party problem. These findings open up new avenues for studying the ongoing dynamics of cognition using EEG and for developing effective and natural brain-computer interfaces.},
    author = {O'Sullivan, James A. and Power, Alan J. and Mesgarani, Nima and Rajaram, Siddharth and Foxe, John J. and Shinn-Cunningham, Barbara G. and Slaney, Malcolm and Shamma, Shihab A. and Lalor, Edmund C.},
    doi = {10.1093/cercor/bht355},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Sullivan et al. - 2015 - Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG.pdf:pdf},
    issn = {14602199},
    journal = {Cerebral Cortex},
    keywords = {BCI,EEG,attention,cocktail party,speech,stimulus-reconstruction},
    mendeley-groups = {First Exam Citations,Alice+Pieman,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {7},
    pages = {1697--1706},
    pmid = {24429136},
    title = {{Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG}},
    volume = {25},
    year = {2015}
}
@article{Vanthornhout2017,
    author = {Vanthornhout, J. and Decruy, L. and Wouters, J. and Simon, J.Z. and Francart, T.},
    doi = {10.1101/246660},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanthornhout et al. - 2017 - Speech intelligibility predicted from neural entrainment of the speech envelope.pdf:pdf},
    journal = {Journal of the Association for Research in Otolaryngology},
    mendeley-groups = {First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {637424},
    title = {{Speech intelligibility predicted from neural entrainment of the speech envelope}},
    year = {2017}
}
@article{ZionGolumbic2013,
abstract = {The ability to focus on and understand one talker in a noisy social environment is a critical social-cognitive capacity, whose underlying neuronal mechanisms are unclear. We investigated the manner in which speech streams are represented in brain activity and the way that selective attention governs the brain's representation of speech using a "Cocktail Party"paradigm, coupled with direct recordings from the cortical surface in surgical epilepsy patients. We find that brain activity dynamically tracks speech streams using both low-frequency phase and high-frequency amplitude fluctuations and that optimal encoding likely combines the two. In and near low-level auditory cortices, attention "modulates"the representation by enhancing cortical tracking of attended speech streams, but ignored speech remains represented. In higher-order regions, the representation appears to become more "selective,"in that there is no detectable tracking of ignored speech. This selectivity itself seems to sharpen as a sentence unfolds. {\textcopyright} 2013 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{Zion Golumbic}, Elana M. and Ding, Nai and Bickel, Stephan and Lakatos, Peter and Schevon, Catherine A. and McKhann, Guy M. and Goodman, Robert R. and Emerson, Ronald and Mehta, Ashesh D. and Simon, Jonathan Z. and Poeppel, David and Schroeder, Charles E.},
doi = {10.1016/j.neuron.2012.12.037},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zion Golumbic et al. - 2013 - Mechanisms underlying selective neuronal tracking of attended speech at a cocktail party.pdf:pdf},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {First Exam Citations,Alice+Pieman,First Exam Citations/Ch4,obama paper 2018,First Exam Citations/presentation,ccn 2019 submission},
number = {5},
pages = {980--991},
pmid = {23473326},
title = {{Mechanisms underlying selective neuronal tracking of attended speech at a "cocktail party"}},
volume = {77},
year = {2013}
}
