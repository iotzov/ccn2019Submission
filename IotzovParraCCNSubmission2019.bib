@article{Haegens2018,
    abstract = {Here we review the role of brain oscillations in sensory processing. We examine the idea that neural entrainment of intrinsic oscillations underlies the processing of rhythmic stimuli in the context of simple isochronous rhythms as well as in music and speech. This has been a topic of growing interest over recent years; however, many issues remain highly controversial: how do fluctuations of intrinsic neural oscillations—both spontaneous and entrained to external stimuli—affect perception, and does this occur automatically or can it be actively controlled by top-down factors? Some of the controversy in the literature stems from confounding use of terminology. Moreover, it is not straightforward how theories and findings regarding isochronous rhythms generalize to more complex, naturalistic stimuli, such as speech and music. Here we aim to clarify terminology, and distinguish between different phenomena that are often lumped together as reflecting “neural entrainment” but may actually vary in their mechanistic underpinnings. Furthermore, we discuss specific caveats and confounds related to making inferences about oscillatory mechanisms from human electrophysiological data.},
    author = {Haegens, Saskia and {Zion Golumbic}, Elana},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Haegens, Zion Golumbic - 2018 - Rhythmic facilitation of sensory processing A critical review.pdf:pdf},
    issn = {18737528},
    journal = {Neuroscience and Biobehavioral Reviews},
    keywords = {Entrainment,Music,Oscillations,Rhythm,Speech,Synchronization},
    mendeley-groups = {First Exam Citations/Ch4,obama paper 2018,First Exam Citations,First Exam Citations/oscillation,First Exam Citations/presentation,ccn 2019 submission},
    number = {December 2017},
    pages = {150--165},
    pmid = {29223770},
    publisher = {Elsevier},
    title = {{Rhythmic facilitation of sensory processing: A critical review}},
    volume = {86},
    year = {2018}
}
@article{Frank2018,
    abstract = {Results from a recent neuroimaging study on spoken sentence comprehension have been interpreted as evidence for cortical entrainment to hierarchical syntactic structure. We present a simple computational model that predicts the power spectra from this study, even though the model's linguistic knowledge is restricted to the lexical level, and word-level representations are not combined into higher-level units (phrases or sentences). Hence, the cortical entrainment results can also be explained from the lexical properties of the stimuli, without recourse to hierarchical syntax.},
    archivePrefix = {arXiv},
    arxivId = {1706.05656},
    author = {Frank, Stefan L. and Yang, Jinbiao},
    eprint = {1706.05656},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Frank, Yang - 2018 - Lexical representation explains cortical entrainment during speech comprehension.pdf:pdf},
    isbn = {1111111111},
    issn = {19326203},
    journal = {PLoS ONE},
    mendeley-groups = {First Exam Citations,obama paper 2018,ccn 2019 submission},
    number = {5},
    pages = {1--11},
    title = {{Lexical representation explains cortical entrainment during speech comprehension}},
    volume = {13},
    year = {2018}
}
@article{Horton2014,
    abstract = {Objective. Recent studies have shown that auditory cortex better encodes the envelope of attended speech than that of unattended speech during multi-speaker ('cocktail party') situations. We investigated whether these differences were sufficiently robust within single-trial electroencephalographic (EEG) data to accurately determine where subjects attended. Additionally, we compared this measure to other established EEG markers of attention. Approach. High-resolution EEG was recorded while subjects engaged in a two-speaker 'cocktail party' task. Cortical responses to speech envelopes were extracted by cross-correlating the envelopes with each EEG channel. We also measured steady-state responses (elicited via high-frequency amplitude modulation of the speech) and alpha-band power, both of which have been sensitive to attention in previous studies. Using linear classifiers, we then examined how well each of these features could be used to predict the subjects' side of attention at various epoch lengths. Main results. We found that the attended speaker could be determined reliably from the envelope responses calculated from short periods of EEG, with accuracy improving as a function of sample length. Furthermore, envelope responses were far better indicators of attention than changes in either alpha power or steady-state responses. Significance. These results suggest that envelope-related signals recorded in EEG data can be used to form robust auditory BCI's that do not require artificial manipulation (e.g., amplitude modulation) of stimuli to function.},
    archivePrefix = {arXiv},
    arxivId = {NIHMS150003},
    author = {Horton, Cort and Srinivasan, Ramesh and D'Zmura, Michael},
    eprint = {NIHMS150003},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Horton, Srinivasan, D'Zmura - 2014 - Envelope responses in single-trial EEG indicate attended speaker in a ‘cocktail party'.pdf:pdf},
    isbn = {2156623929},
    issn = {1741-2560},
    journal = {Journal of Neural Engineering},
    keywords = {alpha lateralization,brain,computer interfaces,selective attention,speech envelopes,steady-state responses},
    mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {4},
    pages = {046015},
    pmid = {24963838},
    publisher = {IOP Publishing},
    title = {{Envelope responses in single-trial EEG indicate attended speaker in a ‘cocktail party'}},
    volume = {11},
    year = {2014}
}
@article{Ding2015,
    abstract = {The most critical attribute of human language is its unbounded combinatorial nature: smaller elements can be combined into larger structures on the basis of a grammatical system, resulting in a hierarchy of linguistic units, such as words, phrases and sentences. Mentally parsing and representing such structures, however, poses challenges for speech comprehension. In speech, hierarchical linguistic structures do not have boundaries that are clearly defined by acoustic cues and must therefore be internally and incrementally constructed during comprehension. We found that, during listening to connected speech, cortical activity of different timescales concurrently tracked the time course of abstract linguistic structures at different hierarchical levels, such as words, phrases and sentences. Notably, the neural tracking of hierarchical linguistic structures was dissociated from the encoding of acoustic cues and from the predictability of incoming words. Our results indicate that a hierarchy of neural processing timescales underlies grammar-based internal construction of hierarchical linguistic structure.},
    archivePrefix = {arXiv},
    arxivId = {15334406},
    author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
    eprint = {15334406},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding et al. - 2015 - Cortical tracking of hierarchical linguistic structures in connected speech.pdf:pdf},
    isbn = {1546-1726},
    issn = {15461726},
    journal = {Nature Neuroscience},
    mendeley-groups = {First Exam Citations,obama paper 2018,First Exam Citations/presentation,ccn 2019 submission},
    number = {1},
    pages = {158--164},
    pmid = {26642090},
    title = {{Cortical tracking of hierarchical linguistic structures in connected speech}},
    volume = {19},
    year = {2015}
}
@article{Dmochowski2017,
    abstract = {In neuroscience, stimulus-response relationships have traditionally been analyzed using either encoding or decoding models. Here we propose a hybrid approach that decomposes neural activity into multiple components, each representing a portion of the stimulus. The technique is implemented via canonical correlation analysis (CCA) by temporally filtering the stimulus (encoding) and spatially filtering the neural responses (decoding) such that the resulting components are maximally correlated. In contrast to existing methods, this approach recovers multiple correlated stimulus-response pairs, and thus affords a richer, multidimensional analysis of neural representations. We first validated the technique's ability to recover multiple stimulus-driven components using electroencephalographic (EEG) data simulated with a finite element model of the head. We then applied the technique to real EEG responses to auditory and audiovisual narratives experienced identically across subjects, as well as uniquely experienced video game play. During narratives, both auditory and visual stimulus-response correlations (SRC) were modulated by attention and tracked inter-subject correlations. During video game play, SRC varied with game difficulty and the presence of a dual task. Interestingly, the strongest component extracted for visual and auditory features of film clips had nearly identical spatial distributions, suggesting that the predominant encephalographic response to naturalistic stimuli is supramodal. The diversity of these findings demonstrates the utility of measuring multidimensional SRC via hybrid encoding-decoding.},
    author = {Dmochowski, Jacek P. and Ki, Jason J. and DeGuzman, Paul and Sajda, Paul and Parra, Lucas C.},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dmochowski et al. - 2017 - Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity.pdf:pdf},
    issn = {10959572},
    journal = {NeuroImage},
    mendeley-groups = {ccn 2019 submission},
    number = {May},
    pages = {1--13},
    publisher = {Elsevier},
    title = {{Extracting multidimensional stimulus-response correlations using hybrid encoding-decoding of neural activity}},
    year = {2017}
}
@article{Ross2007,
    abstract = {Viewing a speaker's articulatory movements substantially improves a listener's ability to understand spoken words, especially under noisy environmental conditions. It has been claimed that this gain is most pronounced when auditory input is weakest, an effect that has been related to a well-known principle of multisensory integration--"inverse effectiveness." In keeping with the predictions of this principle, the present study showed substantial gain in multisensory speech enhancement at even the lowest signal-to-noise ratios (SNRs) used (-24 dB), but it was also evident that there was a "special zone" at a more intermediate SNR of -12 dB where multisensory integration was additionally enhanced beyond the predictions of this principle. As such, we show that inverse effectiveness does not strictly apply to the multisensory enhancements seen during audiovisual speech perception. Rather, the gain from viewing visual articulations is maximal at intermediate SNRs, well above the lowest auditory SNR where the recognition of whole words is significantly different from zero. We contend that the multisensory speech system is maximally tuned for SNRs between extremes, where the system relies on either the visual (speech-reading) or the auditory modality alone, forming a window of maximal integration at intermediate SNR levels. At these intermediate levels, the extent of multisensory enhancement of speech recognition is considerable, amounting to more than a 3-fold performance improvement relative to an auditory-alone condition.},
    author = {Ross, Lars A. and Saint-Amour, Dave and Leavitt, Victoria M. and Javitt, Daniel C. and Foxe, John J.},
    isbn = {1047-3211 (Print)$\backslash$r1047-3211 (Linking)},
    issn = {10473211},
    journal = {Cerebral Cortex},
    keywords = {Audiovisual,Crossmodal,Inverse effectiveness,Lip-reading,Multisensory,Speech perception,Speech-reading},
    mendeley-groups = {First Exam Citations,ccn 2019 submission},
    number = {5},
    pages = {1147--1153},
    pmid = {16785256},
    title = {{Do you see what I am saying? Exploring visual enhancement of speech comprehension in noisy environments}},
    volume = {17},
    year = {2007}
}
@article{Cohen2017,
    abstract = {It is said that we lose track of time -that " time flies " -when we are engrossed in a story. How does engagement with the story cause this distorted perception of time, and what are its neural correlates? People commit both time and attentional resources to an engaging stimulus. For narrative videos, attentional engagement can be represented as the level of similarity between the electroencephalographic responses of different viewers. Here we show that this measure of neural engagement predicted the duration of time that viewers were willing to commit to narrative videos. Contrary to popular wisdom, engagement did not distort the average perception of time duration. Rather, more similar brain responses resulted in a more uniform perception of time across viewers. These findings suggest that by capturing the attention of an audience, narrative videos bring both neural processing and the subjective perception of time into synchrony.},
    author = {Cohen, Samantha S. and Henin, Simon and Parra, Lucas C.},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Henin, Parra - 2017 - Engaging narratives evoke similar neural activity and lead to similar time perception(2).pdf:pdf},
    issn = {20452322},
    journal = {Scientific Reports},
    mendeley-groups = {First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {1},
    pages = {1--10},
    publisher = {Springer US},
    title = {{Engaging narratives evoke similar neural activity and lead to similar time perception}},
    volume = {7},
    year = {2017}
}
@article{Luo2007,
    abstract = {How natural speech is represented in the auditory cortex constitutes a major challenge for cognitive neuroscience. Although many single-unit and neuroimaging studies have yielded valuable insights about the processing of speech and matched complex sounds, the mechanisms underlying the analysis of speech dynamics in human auditory cortex remain largely unknown. Here, we show that the phase pattern of theta band (4-8 Hz) responses recorded from human auditory cortex with magnetoencephalography (MEG) reliably tracks and discriminates spoken sentences and that this discrimination ability is correlated with speech intelligibility. The findings suggest that an ???200 ms temporal window (period of theta oscillation) segments the incoming speech signal, resetting and sliding to track speech dynamics. This hypothesized mechanism for cortical speech analysis is based on the stimulus-induced modulation of inherent cortical rhythms and provides further evidence implicating the syllable as a computational primitive for the representation of spoken language. ?? 2007 Elsevier Inc. All rights reserved.},
    archivePrefix = {arXiv},
    arxivId = {NIHMS150003},
    author = {Luo, Huan and Poeppel, David},
    eprint = {NIHMS150003},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Poeppel - 2007 - Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex.pdf:pdf},
    isbn = {2122633255},
    issn = {08966273},
    journal = {Neuron},
    keywords = {SYSNEURO},
    mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {6},
    pages = {1001--1010},
    pmid = {17582338},
    title = {{Phase Patterns of Neuronal Responses Reliably Discriminate Speech in Human Auditory Cortex}},
    volume = {54},
    year = {2007}
}
@article{Crosse2016,
    abstract = {UNLABELLED Speech comprehension is improved by viewing a speaker's face, especially in adverse hearing conditions, a principle known as inverse effectiveness. However, the neural mechanisms that help to optimize how we integrate auditory and visual speech in such suboptimal conversational environments are not yet fully understood. Using human EEG recordings, we examined how visual speech enhances the cortical representation of auditory speech at a signal-to-noise ratio that maximized the perceptual benefit conferred by multisensory processing relative to unisensory processing. We found that the influence of visual input on the neural tracking of the audio speech signal was significantly greater in noisy than in quiet listening conditions, consistent with the principle of inverse effectiveness. Although envelope tracking during audio-only speech was greatly reduced by background noise at an early processing stage, it was markedly restored by the addition of visual speech input. In background noise, multisensory integration occurred at much lower frequencies and was shown to predict the multisensory gain in behavioral performance at a time lag of ∼250 ms. Critically, we demonstrated that inverse effectiveness, in the context of natural audiovisual (AV) speech processing, relies on crossmodal integration over long temporal windows. Our findings suggest that disparate integration mechanisms contribute to the efficient processing of AV speech in background noise. SIGNIFICANCE STATEMENT The behavioral benefit of seeing a speaker's face during conversation is especially pronounced in challenging listening environments. However, the neural mechanisms underlying this phenomenon, known as inverse effectiveness, have not yet been established. Here, we examine this in the human brain using natural speech-in-noise stimuli that were designed specifically to maximize the behavioral benefit of audiovisual (AV) speech. We find that this benefit arises from our ability to integrate multimodal information over longer periods of time. Our data also suggest that the addition of visual speech restores early tracking of the acoustic speech signal during excessive background noise. These findings support and extend current mechanistic perspectives on AV speech perception.},
    author = {Crosse, M. J. and {Di Liberto}, G. M. and Lalor, E. C.},
    isbn = {0270-6474},
    issn = {0270-6474},
    journal = {Journal of Neuroscience},
    mendeley-groups = {obama paper 2018,ccn 2019 submission},
    number = {38},
    pages = {9888--9895},
    pmid = {27656026},
    title = {{Eye Can Hear Clearly Now: Inverse Effectiveness in Natural Audiovisual Speech Processing Relies on Long-Term Crossmodal Temporal Integration}},
    volume = {36},
    year = {2016}
}
@article{Ki2016,
    abstract = {UNLABELLED: Attentional engagement is a major determinant of how effectively we gather information through our senses. Alongside the sheer growth in the amount and variety of information content that we are presented with through modern media, there is increased variability in the degree to which we "absorb" that information. Traditional research on attention has illuminated the basic principles of sensory selection to isolated features or locations, but it provides little insight into the neural underpinnings of our attentional engagement with modern naturalistic content. Here, we show in human subjects that the reliability of an individual's neural responses with respect to a larger group provides a highly robust index of the level of attentional engagement with a naturalistic narrative stimulus. Specifically, fast electroencephalographic evoked responses were more strongly correlated across subjects when naturally attending to auditory or audiovisual narratives than when attention was directed inward to a mental arithmetic task during stimulus presentation. This effect was strongest for audiovisual stimuli with a cohesive narrative and greatly reduced for speech stimuli lacking meaning. For compelling audiovisual narratives, the effect is remarkably strong, allowing perfect discrimination between attentional state across individuals. Control experiments rule out possible confounds related to altered eye movement trajectories or order of presentation. We conclude that reliability of evoked activity reproduced across subjects viewing the same movie is highly sensitive to the attentional state of the viewer and listener, which is aided by a cohesive narrative.$\backslash$n$\backslash$nSIGNIFICANCE STATEMENT: Modern technology has brought about a monumental surge in the availability of information. There is a pressing need for ways to measure and characterize attentional engagement and understand the causes of its waxing and waning in natural settings. The present study demonstrates that the degree of an individual's attentional engagement with naturalistic narrative stimuli is strongly predicted by the degree of similarity of his/her neural responses to a wider group. The effect is so strong that it enables perfect classification of attentional state across individuals for some narrative stimuli. As modern information content continues to diversify, such direct neural metrics will become increasingly important to manage and evaluate its effects on both the healthy and disordered human brain.},
    author = {Ki, J. J. and Kelly, S. P. and Parra, L. C.},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ki, Kelly, Parra - 2016 - Attention Strongly Modulates Reliability of Neural Responses to Naturalistic Narrative Stimuli.pdf:pdf},
    isbn = {0270-6474},
    issn = {0270-6474},
    journal = {Journal of Neuroscience},
    keywords = {attention,inter-subject correlation,naturalistic narrative stimuli},
    mendeley-groups = {Alice+Pieman,cns2018 poster,obama paper 2018,ccn 2019 submission},
    number = {10},
    pages = {3092--3101},
    pmid = {26961961},
    title = {{Attention Strongly Modulates Reliability of Neural Responses to Naturalistic Narrative Stimuli}},
    volume = {36},
    year = {2016}
}
@article{Crosse2015,
    author = {Crosse, M. J. and Butler, J. S. and Lalor, E. C.},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Crosse, Butler, Lalor - 2015 - Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditi.pdf:pdf},
    isbn = {0270-6474},
    issn = {0270-6474},
    journal = {Journal of Neuroscience},
    mendeley-groups = {First Exam Citations,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {42},
    pages = {14195--14204},
    pmid = {26490860},
    title = {{Congruent Visual Speech Enhances Cortical Entrainment to Continuous Auditory Speech in Noise-Free Conditions}},
    volume = {35},
    year = {2015}
}
@article{OSullivan2015,
    abstract = {How humans solve the cocktail party problem remains unknown. However, progress has been made recently thanks to the realization that cortical activity tracks the amplitude envelope of speech. This has led to the development of regression methods for studying the neurophysiology of continuous speech. One such method, known as stimulus-reconstruction, has been successfully utilized with cortical surface recordings and magnetoencephalography (MEG). However, the former is invasive and gives a relatively restricted view of processing along the auditory hierarchy, whereas the latter is expensive and rare. Thus it would be extremely useful for research in many populations if stimulus-reconstruction was effective using electroencephalography (EEG), a widely available and inexpensive technology. Here we show that single-trial (≈60 s) unaveraged EEG data can be decoded to determine attentional selection in a naturalistic multispeaker environment. Furthermore, we show a significant correlation between our EEG-based measure of attention and performance on a high-level attention task. In addition, by attempting to decode attention at individual latencies, we identify neural processing at ∼200 ms as being critical for solving the cocktail party problem. These findings open up new avenues for studying the ongoing dynamics of cognition using EEG and for developing effective and natural brain-computer interfaces.},
    author = {O'Sullivan, James A. and Power, Alan J. and Mesgarani, Nima and Rajaram, Siddharth and Foxe, John J. and Shinn-Cunningham, Barbara G. and Slaney, Malcolm and Shamma, Shihab A. and Lalor, Edmund C.},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Sullivan et al. - 2015 - Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG.pdf:pdf},
    issn = {14602199},
    journal = {Cerebral Cortex},
    keywords = {BCI,EEG,attention,cocktail party,speech,stimulus-reconstruction},
    mendeley-groups = {First Exam Citations,Alice+Pieman,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {7},
    pages = {1697--1706},
    pmid = {24429136},
    title = {{Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG}},
    volume = {25},
    year = {2015}
}
@article{Peelle2013,
    abstract = {A growing body of evidence shows that ongoing oscillations in auditory cortex modulate their phase to match the rhythm of temporally regular acoustic stimuli, increasing sensitivity to relevant environmental cues and improving detection accuracy. In the current study, we test the hypothesis that nonsensory information provided by linguistic content enhances phase-locked responses to intelligible speech in the human brain. Sixteen adults listened to meaningful sentences while we recorded neural activity using magnetoencephalography. Stimuli were processed using a noise-vocoding technique to vary intelligibility while keeping the temporal acoustic envelope consistent. We show that the acoustic envelopes of sentences contain most power between 4 and 7 Hz and that it is in this frequency band that phase locking between neural activity and envelopes is strongest. Bilateral oscillatory neural activity phase-locked to unintelligible speech, but this cerebro-acoustic phase locking was enhanced when speech was intelligible. This enhanced phase locking was left lateralized and localized to left temporal cortex. Together, our results demonstrate that entrainment to connected speech does not only depend on acoustic characteristics, but is also affected by listeners' ability to extract linguistic information. This suggests a biological framework for speech comprehension in which acoustic and linguistic cues reciprocally aid in stimulus prediction.},
    author = {Peelle, Jonathan E. and Gross, Joachim and Davis, Matthew H.},
    file = {:home/ivan/Downloads/bhs118.pdf:pdf},
    isbn = {1460-2199 (Electronic)$\backslash$n1047-3211 (Linking)},
    issn = {14602199},
    journal = {Cerebral Cortex},
    keywords = {MEG,entrainment,intelligibility,prediction,rhythm,speech comprehension},
    mendeley-groups = {First Exam Citations,ccn 2019 submission},
    number = {6},
    pages = {1378--1387},
    pmid = {22610394},
    title = {{Phase-locked responses to speech in human auditory cortex are enhanced during comprehension}},
    volume = {23},
    year = {2013}
}
@article{Ding2014a,
    abstract = {Auditory cortical activity is entrained to the temporal envelope of speech, which corresponds to the syllabic rhythm of speech. Such entrained cortical activity can be measured from subjects naturally listening to sentences or spoken passages, providing a reliable neural marker of online speech processing. A central question still remains to be answered about whether cortical entrained activity is more closely related to speech perception or non-speech-specific auditory encoding. Here, we review a few hypotheses about the functional roles of cortical entrainment to speech, e.g., encoding acoustic features, parsing syllabic boundaries, and selecting sensory information in complex listening environments. It is likely that speech entrainment is not a homogeneous response and these hypotheses apply separately for speech entrainment generated from different neural sources. The relationship between entrained activity and speech intelligibility is also discussed. A tentative conclusion is that theta-band entrainment (4-8 Hz) encodes speech features critical for intelligibility while delta-band entrainment (1-4 Hz) is related to the perceived, non-speech-specific acoustic rhythm. To further understand the functional properties of speech entrainment, a splitter's approach will be needed to investigate (1) not just the temporal envelope but what specific acoustic features are encoded and (2) not just speech intelligibility but what specific psycholinguistic processes are encoded by entrained cortical activity. Similarly, the anatomical and spectro-temporal details of entrained activity need to be taken into account when investigating its functional properties.},
    author = {Ding, Nai and Simon, Jonathan Z.},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ding, Simon - 2014 - Cortical entrainment to continuous speech functional roles and interpretations.pdf:pdf},
    issn = {1662-5161},
    journal = {Frontiers in Human Neuroscience},
    mendeley-groups = {First Exam Citations,Alice+Pieman,First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    pmid = {24904354},
    title = {{Cortical entrainment to continuous speech: functional roles and interpretations}},
    volume = {8},
    year = {2014}
}
@article{ZionGolumbic2013,
abstract = {The ability to focus on and understand one talker in a noisy social environment is a critical social-cognitive capacity, whose underlying neuronal mechanisms are unclear. We investigated the manner in which speech streams are represented in brain activity and the way that selective attention governs the brain's representation of speech using a "Cocktail Party"paradigm, coupled with direct recordings from the cortical surface in surgical epilepsy patients. We find that brain activity dynamically tracks speech streams using both low-frequency phase and high-frequency amplitude fluctuations and that optimal encoding likely combines the two. In and near low-level auditory cortices, attention "modulates"the representation by enhancing cortical tracking of attended speech streams, but ignored speech remains represented. In higher-order regions, the representation appears to become more "selective,"in that there is no detectable tracking of ignored speech. This selectivity itself seems to sharpen as a sentence unfolds. {\textcopyright} 2013 Elsevier Inc.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {{Zion Golumbic}, Elana M. and Ding, Nai and Bickel, Stephan and Lakatos, Peter and Schevon, Catherine A. and McKhann, Guy M. and Goodman, Robert R. and Emerson, Ronald and Mehta, Ashesh D. and Simon, Jonathan Z. and Poeppel, David and Schroeder, Charles E.},
eprint = {NIHMS150003},
file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zion Golumbic et al. - 2013 - Mechanisms underlying selective neuronal tracking of attended speech at a cocktail party.pdf:pdf},
issn = {08966273},
journal = {Neuron},
mendeley-groups = {First Exam Citations,Alice+Pieman,First Exam Citations/Ch4,obama paper 2018,First Exam Citations/presentation,ccn 2019 submission},
number = {5},
pages = {980--991},
pmid = {23473326},
title = {{Mechanisms underlying selective neuronal tracking of attended speech at a "cocktail party"}},
volume = {77},
year = {2013}
}
@article{Vanthornhout2017,
    author = {Vanthornhout, J. and Decruy, L. and Wouters, J. and Simon, J.Z. and Francart, T.},
    file = {:home/ivan/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Vanthornhout et al. - 2017 - Speech intelligibility predicted from neural entrainment of the speech envelope.pdf:pdf},
    journal = {Journal of the Association for Research in Otolaryngology},
    mendeley-groups = {First Exam Citations/Ch4,obama paper 2018,ccn 2019 submission},
    number = {637424},
    title = {{Speech intelligibility predicted from neural entrainment of the speech envelope}},
    year = {2017}
}
@article{Ding2013,
abstract = {Speech recognition is remarkably robust to the listening background, even when the energy of background sounds strongly overlaps with that of speech. How the brain transforms the corrupted acoustic signal into a reliable neural representation suitable for speech recognition, however, remains elusive. Here, we hypothesize that this transformation is performed at the level of auditory cortex through adaptive neural encoding, and we test the hypothesis by recording, using MEG, the neural responses of human subjects listening to a narrated story. Spectrally matched stationary noise, which has maximal acoustic overlap with the speech, is mixed in at various intensity levels. Despite the severe acoustic interference caused by this noise, it is here demonstrated that low-frequency auditory cortical activity is reliably synchronized to the slow temporal modulations of speech, even when the noise is twice as strong as the speech. Such a reliable neural representation is maintained by intensity contrast gain control and by adaptive processing of temporal modulations at different time scales, corresponding to the neural {\{}delta{\}} and {\{}theta{\}} bands. Critically, the precision of this neural synchronization predicts how well a listener can recognize speech in noise, indicating that the precision of the auditory cortical representation limits the performance of speech recognition in noise. Together, these results suggest that, in a complex listening environment, auditory cortex can selectively encode a speech stream in a background insensitive manner, and this stable neural representation of speech provides a plausible basis for background-invariant recognition of speech.},
author = {Ding, N. and Simon, J. Z.},
issn = {0270-6474},
journal = {Journal of Neuroscience},
mendeley-groups = {ccn 2019 submission},
number = {13},
pages = {5728--5735},
title = {{Adaptive Temporal Encoding Leads to a Background-Insensitive Cortical Representation of Speech}},
volume = {33},
year = {2013}
}
